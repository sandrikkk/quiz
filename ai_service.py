import asyncio
import aiohttp
import json
import logging
import uuid
from datetime import datetime
from typing import Optional
from config import GOOGLE_API_KEY, ENABLE_AI_EXPLANATIONS, GEMINI_API_URL, GEMINI_MODEL

# Setup logging for API requests
logger = logging.getLogger(__name__)
if not logger.handlers:
    handler = logging.StreamHandler()
    formatter = logging.Formatter('%(levelname)s:%(name)s:%(message)s')
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    logger.setLevel(logging.INFO)
    logger.propagate = False

class AIExplanationService:
    def __init__(self):
        self.api_key = GOOGLE_API_KEY
        self.enabled = ENABLE_AI_EXPLANATIONS and bool(self.api_key)
        self.request_count = 0
        self.total_questions_processed = 0
    
    async def get_batch_explanations(self, incorrect_questions: list) -> dict:
        """Generate AI explanations for multiple incorrect answers in one request"""
        if not self.enabled or not incorrect_questions:
            logger.info(f"AI disabled or no questions. Questions count: {len(incorrect_questions)}")
            return {self._get_question_key(q): self._get_default_explanation(q["user_answer"], q["correct_answer"]) 
                    for q in incorrect_questions}
        
        # Generate unique request ID
        request_id = str(uuid.uuid4())[:8]
        
        # Log request details
        self.request_count += 1
        self.total_questions_processed += len(incorrect_questions)
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        logger.info(f"üöÄ GEMINI API REQUEST #{self.request_count} [ID: {request_id}]")
        logger.info(f"üìÖ Time: {timestamp}")
        logger.info(f"ü§ñ Model: {GEMINI_MODEL}")
        logger.info(f"‚ùì Questions in this batch: {len(incorrect_questions)}")
        logger.info(f"üìä Total questions processed so far: {self.total_questions_processed}")
        logger.info(f"üîß API Key configured: {'Yes' if self.api_key else 'No'}")
        
        try:
            prompt = self._create_batch_prompt(incorrect_questions)
            
            # Log prompt size
            prompt_size = len(prompt)
            logger.info(f"üìù Prompt size: {prompt_size} characters")
            
            async with aiohttp.ClientSession() as session:
                headers = {
                    'Content-Type': 'application/json',
                }
                
                payload = {
                    "contents": [{
                        "parts": [{
                            "text": prompt
                        }]
                    }]
                }
                
                url = f"{GEMINI_API_URL}?key={self.api_key}"
                
                logger.info(f"üåê Sending request to Gemini API... [ID: {request_id}]")
                
                async with session.post(url, headers=headers, json=payload) as response:
                    response_time = datetime.now().strftime("%H:%M:%S")
                    
                    if response.status == 200:
                        data = await response.json()
                        if data.get('candidates') and len(data['candidates']) > 0:
                            content = data['candidates'][0].get('content', {})
                            if content.get('parts') and len(content['parts']) > 0:
                                ai_response = content['parts'][0].get('text', '').strip()
                                response_size = len(ai_response)
                                
                                logger.info(f"‚úÖ SUCCESS at {response_time} [ID: {request_id}]")
                                logger.info(f"üìÑ Response size: {response_size} characters")
                                logger.info(f"üí∞ Cost estimate: ~${self._estimate_cost(prompt_size, response_size):.4f}")
                                logger.info(f"üèÅ REQUEST #{self.request_count} COMPLETED [ID: {request_id}]")
                                
                                return self._parse_batch_response(ai_response, incorrect_questions)
                    else:
                        error_text = await response.text()
                        logger.error(f"‚ùå API ERROR at {response_time} [ID: {request_id}]")
                        logger.error(f"üî¥ Status: {response.status}")
                        logger.error(f"üìã Error: {error_text}")
                        
                        return {self._get_question_key(q): self._get_default_explanation(q["user_answer"], q["correct_answer"]) 
                                for q in incorrect_questions}
                        
        except Exception as e:
            logger.error(f"üí• EXCEPTION: {str(e)}")
            return {self._get_question_key(q): self._get_default_explanation(q["user_answer"], q["correct_answer"]) 
                    for q in incorrect_questions}
    
    def _get_question_key(self, question_info: dict) -> str:
        """Generate unique key for question"""
        return f"{question_info['question_data']['id']}_{question_info['user_answer']}_{question_info['correct_answer']}"
    
    def _get_default_explanation(self, user_answer: str, correct_answer: str) -> str:
        """Generate a simple default explanation when AI is unavailable"""
        return f"""**·Éó·É•·Éï·Éî·Éú·Éò ·Éû·Éê·É°·É£·ÉÆ·Éò:** {user_answer} ‚ùå  
**·É°·É¨·Éù·É†·Éò ·Éû·Éê·É°·É£·ÉÆ·Éò:** {correct_answer} ‚úÖ

üí° **·É†·É©·Éî·Éï·Éê:** ·Éí·Éê·Éì·Éê·ÉÆ·Éî·Éì·Éî·Éó ·É°·Éê·É°·É¨·Éê·Éï·Éö·Éù ·Éõ·Éê·É°·Éê·Éö·Éê·É° ·Éê·Éõ ·Éó·Éî·Éõ·Éê·Éñ·Éî."""
    
    def _create_batch_prompt(self, incorrect_questions: list) -> str:
        """Create batch prompt for multiple questions"""
        option_labels = ['A', 'B', 'C', 'D']
        
        questions_text = ""
        for i, q_info in enumerate(incorrect_questions, 1):
            question = q_info["question_data"]
            user_answer = q_info["user_answer"]
            correct_answer = q_info["correct_answer"]
            
            options_text = ""
            for j, option in enumerate(question["options"]):
                options_text += f"{option_labels[j]}. {option}\n"
            
            questions_text += f"""
---
**·Éô·Éò·Éó·ÉÆ·Éï·Éê {i} (ID: {question['id']}):**
{question['question']}

**·Éï·Éê·É†·Éò·Éê·Éú·É¢·Éî·Éë·Éò:**
{options_text}
**·Éõ·Éù·É°·É¨·Éê·Éï·Éö·Éò·É° ·Éû·Éê·É°·É£·ÉÆ·Éò:** {user_answer}
**·É°·É¨·Éù·É†·Éò ·Éû·Éê·É°·É£·ÉÆ·Éò:** {correct_answer}
"""
        
        prompt = f"""·Éó·É•·Éï·Éî·Éú ·ÉÆ·Éê·É†·Éó IT ·Éí·Éê·Éú·Éê·Éó·Éö·Éî·Éë·Éò·É° ·Éî·É•·É°·Éû·Éî·É†·É¢·Éò ·Éì·Éê ·Éû·É†·Éù·Éí·É†·Éê·Éõ·Éò·É†·Éî·Éë·Éò·É° ·Éò·Éú·É°·É¢·É†·É£·É•·É¢·Éù·É†·Éò. ·É•·Éï·Éî·Éõ·Éù·Éó ·Éõ·Éù·É™·Éî·Éõ·É£·Éö·Éò ·Éß·Éï·Éî·Éö·Éê ·Éô·Éò·Éó·ÉÆ·Éï·Éê ·Éì·Éê·Éô·Éê·Éï·É®·Éò·É†·Éî·Éë·É£·Éö·Éò·Éê ·Éò·Éú·É§·Éù·É†·Éõ·Éê·É™·Éò·É£·Éö ·É¢·Éî·É•·Éú·Éù·Éö·Éù·Éí·Éò·Éî·Éë·Éó·Éê·Éú, ·Éû·É†·Éù·Éí·É†·Éê·Éõ·Éò·É†·Éî·Éë·Éê·É°·Éó·Éê·Éú, ·Éô·Éù·Éõ·Éû·Éò·É£·É¢·Éî·É†·É£·Éö ·Éõ·Éî·É™·Éú·Éò·Éî·É†·Éî·Éë·Éê·É°·Éó·Éê·Éú ·Éì·Éê ·É•·É°·Éî·Éö·É£·É† ·É¢·Éî·É•·Éú·Éù·Éö·Éù·Éí·Éò·Éî·Éë·Éó·Éê·Éú.

·Éí·Éó·ÉÆ·Éù·Éï·Éó ·Éê·ÉÆ·É°·Éú·Éê·Éó ·É•·Éê·É†·Éó·É£·Éö ·Éî·Éú·Éê·Éñ·Éî ·É†·Éê·É¢·Éù·Éõ ·Éê·É†·Éò·É° ·Éê·É†·Éê·É°·É¨·Éù·É†·Éò ·Éó·Éò·Éó·Éù·Éî·É£·Éö·Éò ·Éõ·Éù·É™·Éî·Éõ·É£·Éö·Éò ·Éû·Éê·É°·É£·ÉÆ·Éò, ·Éí·Éê·Éõ·Éù·Éò·Éß·Éî·Éú·Éî·Éó IT ·É¢·Éî·É†·Éõ·Éò·Éú·Éù·Éö·Éù·Éí·Éò·Éê ·Éì·Éê ·É¢·Éî·É•·Éú·Éò·Éô·É£·É†·Éò ·É™·Éù·Éì·Éú·Éê.

{questions_text}

**·Éõ·Éù·Éó·ÉÆ·Éù·Éï·Éú·Éî·Éë·Éò:**
1. ·Éó·Éò·Éó·Éù·Éî·É£·Éö·Éò ·Éô·Éò·Éó·ÉÆ·Éï·Éò·É°·Éó·Éï·Éò·É° ·Éõ·Éù·Éô·Éö·Éî, ·Éö·Éê·Éô·Éù·Éú·É£·É†·Éò ·Éê·ÉÆ·É°·Éú·Éê
2. ·Éõ·Éù·Éò·É™·Éê·Éï·Éì·Éî·É° ·É†·Éê·É¢·Éù·Éõ ·Éê·É†·Éò·É° ·É°·É¨·Éù·É†·Éò ·É°·É¨·Éù·É†·Éò ·Éû·Éê·É°·É£·ÉÆ·Éò (IT ·É¢·Éî·É•·Éú·Éò·Éô·É£·É†·Éò ·Éó·Éï·Éê·Éö·É°·Éê·Éñ·É†·Éò·É°·Éò·Éó)
3. ·É†·Éê·É¢·Éù·Éõ ·Éê·É†·Éò·É° ·Éê·É†·Éê·É°·É¨·Éù·É†·Éò ·Éõ·Éù·É°·É¨·Éê·Éï·Éö·Éò·É° ·Éû·Éê·É°·É£·ÉÆ·Éò (·É¢·Éî·É•·Éú·Éò·Éô·É£·É†·Éò ·Éí·Éê·Éú·Éõ·Éê·É†·É¢·Éî·Éë·Éò·Éó)
4. ·Éí·Éê·Éõ·Éù·Éò·Éß·Éî·Éú·Éî·Éó markdown ·É§·Éù·É†·Éõ·Éê·É¢·Éò·É†·Éî·Éë·Éê
5. ·Éõ·Éê·É•·É°·Éò·Éõ·É£·Éõ 2-3 ·É¨·Éò·Éú·Éê·Éì·Éê·Éì·Éî·Éë·Éê ·Éó·Éò·Éó·Éù ·Éô·Éò·Éó·ÉÆ·Éï·Éê·Éñ·Éî
6. ·Éí·Éê·Éõ·Éù·Éò·Éß·Éî·Éú·Éî·Éó ·É•·Éê·É†·Éó·É£·Éö·Éò IT ·É¢·Éî·É†·Éõ·Éò·Éú·Éù·Éö·Éù·Éí·Éò·Éê ·Éì·Éê ·É¢·Éî·É•·Éú·Éò·Éô·É£·É†·Éò ·Éî·Éú·Éê

**·É§·Éù·É†·Éõ·Éê·É¢·Éò:**
```
### ·Éô·Éò·Éó·ÉÆ·Éï·Éê 1 (ID: X)
**·É°·É¨·Éù·É†·Éò ·Éû·Éê·É°·É£·ÉÆ·Éò (Y):** [IT ·É¢·Éî·É•·Éú·Éò·Éô·É£·É†·Éò ·Éê·ÉÆ·É°·Éú·Éê]
**·É†·Éê·É¢·Éù·Éõ·Éê·Éê ·Éê·É†·Éê·É°·É¨·Éù·É†·Éò (Z):** [·É¢·Éî·É•·Éú·Éò·Éô·É£·É†·Éò ·Éí·Éê·Éú·Éõ·Éê·É†·É¢·Éî·Éë·Éê]

### ·Éô·Éò·Éó·ÉÆ·Éï·Éê 2 (ID: X)
...
```"""
        
        return prompt
    
    def _parse_batch_response(self, ai_response: str, incorrect_questions: list) -> dict:
        """Parse AI response and map to question keys"""
        explanations = {}
        
        # Split response by question sections
        sections = ai_response.split("### ·Éô·Éò·Éó·ÉÆ·Éï·Éê")
        
        for i, section in enumerate(sections[1:], 1):  # Skip first empty section
            try:
                # Extract question ID from section
                id_match = section.split("(ID: ")[1].split(")")[0] if "(ID: " in section else None
                
                if id_match:
                    question_id = int(id_match)
                    # Find matching question
                    for q_info in incorrect_questions:
                        if q_info["question_data"]["id"] == question_id:
                            key = self._get_question_key(q_info)
                            # Clean up the explanation text
                            explanation = f"### ·Éô·Éò·Éó·ÉÆ·Éï·Éê {i} (ID: {question_id})\n" + section.split(f"(ID: {question_id})")[1].strip()
                            explanations[key] = explanation
                            break
            except Exception as e:
                print(f"Error parsing question section {i}: {e}")
                continue
        
        # Fill in default explanations for any missing questions
        for q_info in incorrect_questions:
            key = self._get_question_key(q_info)
            if key not in explanations:
                explanations[key] = self._get_default_explanation(q_info["user_answer"], q_info["correct_answer"])
        
        return explanations
    
    def _estimate_cost(self, prompt_size: int, response_size: int) -> float:
        """Estimate API cost based on character count (approximate)"""
        # Gemini 2.0 Flash pricing (approximate): $0.000075 per 1K characters for input, $0.0003 per 1K characters for output
        input_cost = (prompt_size / 1000) * 0.000075
        output_cost = (response_size / 1000) * 0.0003
        return input_cost + output_cost
    
    def get_statistics(self) -> dict:
        """Get API usage statistics"""
        return {
            "total_requests": self.request_count,
            "total_questions_processed": self.total_questions_processed,
            "enabled": self.enabled,
            "api_key_configured": bool(self.api_key)
        }
    
    def reset_statistics(self):
        """Reset API usage statistics"""
        old_count = self.request_count
        old_questions = self.total_questions_processed
        self.request_count = 0
        self.total_questions_processed = 0
        logger.info(f"üîÑ STATISTICS RESET: {old_count} requests, {old_questions} questions")
    
    def _create_prompt(self, question: str, options: list, user_answer: str, correct_answer: str) -> str:
        """Create a prompt for AI explanation generation"""
        option_labels = ['A', 'B', 'C', 'D']
        options_text = ""
        for i, option in enumerate(options):
            options_text += f"{option_labels[i]}. {option}\n"
        
        prompt = f"""·Éó·É•·Éï·Éî·Éú ·ÉÆ·Éê·É†·Éó ·Éí·Éê·Éú·Éê·Éó·Éö·Éî·Éë·Éò·É° ·Éî·É•·É°·Éû·Éî·É†·É¢·Éò. ·Éí·Éó·ÉÆ·Éù·Éï·Éó ·Éê·ÉÆ·É°·Éú·Éê·Éó ·É•·Éê·É†·Éó·É£·Éö ·Éî·Éú·Éê·Éñ·Éî ·É†·Éê·É¢·Éù·Éõ ·Éê·É†·Éò·É° ·Éê·É†·Éê·É°·É¨·Éù·É†·Éò ·Éõ·Éù·É™·Éî·Éõ·É£·Éö·Éò ·Éû·Éê·É°·É£·ÉÆ·Éò ·Éì·Éê ·É†·Éê·É¢·Éù·Éõ ·Éê·É†·Éò·É° ·É°·É¨·Éù·É†·Éò ·É°·É¨·Éù·É†·Éò ·Éû·Éê·É°·É£·ÉÆ·Éò.

·Éô·Éò·Éó·ÉÆ·Éï·Éê: {question}

·Éï·Éê·É†·Éò·Éê·Éú·É¢·Éî·Éë·Éò:
{options_text}

·Éõ·Éù·É°·É¨·Éê·Éï·Éö·Éò·É° ·Éû·Éê·É°·É£·ÉÆ·Éò: {user_answer}
·É°·É¨·Éù·É†·Éò ·Éû·Éê·É°·É£·ÉÆ·Éò: {correct_answer}

·Éí·Éó·ÉÆ·Éù·Éï·Éó ·Éõ·Éò·Éê·É¨·Éù·Éì·Éù·Éó:
1. ·É†·Éê·É¢·Éù·Éõ ·Éê·É†·Éò·É° ·É°·É¨·Éù·É†·Éò "{correct_answer}" ·Éû·Éê·É°·É£·ÉÆ·Éò
2. ·É†·Éê·É¢·Éù·Éõ ·Éê·É†·Éò·É° ·Éê·É†·Éê·É°·É¨·Éù·É†·Éò "{user_answer}" ·Éû·Éê·É°·É£·ÉÆ·Éò  
3. ·Éõ·Éù·Éô·Éö·Éî ·Éê·ÉÆ·É°·Éú·Éê ·É°·ÉÆ·Éï·Éê ·Éï·Éê·É†·Éò·Éê·Éú·É¢·Éî·Éë·Éò·É° ·É®·Éî·É°·Éê·ÉÆ·Éî·Éë

·Éê·ÉÆ·É°·Éú·Éê ·É£·Éú·Éì·Éê ·Éò·Éß·Éù·É° ·Éõ·Éô·Éê·É§·Éò·Éù, ·Éõ·Éê·É†·É¢·Éò·Éï·Éò ·Éì·Éê ·Éí·Éê·Éú·Éõ·Éê·Éú·Éê·Éó·Éö·Éî·Éë·Éî·Éö·Éò. ·Éí·Éê·Éõ·Éù·Éò·Éß·Éî·Éú·Éî·Éó ·É•·Éê·É†·Éó·É£·Éö·Éò ·É¢·Éî·É•·Éú·Éò·Éô·É£·É†·Éò ·É¢·Éî·É†·Éõ·Éò·Éú·Éù·Éö·Éù·Éí·Éò·Éê."""

        return prompt

# Global instance
ai_service = AIExplanationService() 